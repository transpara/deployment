version: "3.9"
   
services:

  tstore-database:
    labels:
      - "Version=${TSTORE_TAG}"
      - "ID=${TSTORE_ID}"
    image: "transpara/tstore-database:${TSTORE_TAG}"
#    shm_size: '2gb'
    volumes:
      - tstore_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - PGDATA=/var/lib/postgresql/data
      - PGPORT=5432
      - TSTUNE_PROFILE=promscale # this is the profile for timescaledb-tune to optimize postgres for promscale-like loads (wal size, buffers, etc)
    container_name: ${TSTORE_NAME}_db
    hostname: ${TSTORE_NAME}_db
    ports:
      - "${TSTORE_PORT}:5432"
    restart: unless-stopped


  tstore-pool:
    image: "bitnami/pgbouncer:latest"
    environment:
      - POSTGRESQL_USERNAME=${POSTGRES_USER}
      - POSTGRESQL_PASSWORD=${POSTGRES_PASSWORD}  
      - POSTGRESQL_HOST=${TSTORE_NAME}_db
      - POSTGRESQL_DATABASE=postgres
      - PGBOUNCER_PORT=6432
      # we will assume the following configuration here:
      # max_connections in postgress = 500
      # 8 workers for tstore_interface
      # db writes bypass pgbouncer, so we leave 6*8=48 connections for the writes
      - PGBOUNCER_POOL_MODE=transaction #it allows reusing connections between transactiosn
      - PGBOUNCER_MAX_CLIENT_CONN=800 #100 connections per client process as a buffer
      - PGBOUNCER_DEFAULT_POOL_SIZE=30 #Assuming each client process won't need more than a few connections to the database at a time
      - PGBOUNCER_RESERVE_POOL_SIZE=70 #we assume max 100 connections per each worker, so 30 default + 70 in reserve.
      - PGBOUNCER_QUERY_WAIT_TIMEOUT=300
      - PGBOUNCER_MAX_DB_CONNECTIONS=450
    container_name: ${TSTORE_NAME}_pool
    hostname: ${TSTORE_NAME}_pool          


networks:
   default:
      name: transpara
      external: true

volumes:
   tstore_data:
      name: tstore_db_data_${TSTORE_ID}
